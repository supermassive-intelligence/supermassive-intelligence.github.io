# ScalarLM Benchmarking MI300X BF16 GEMM

This blog covers the performance of the MI300X GPU in the context of a bfloat16 GEMM benchmark.

Here are the specs of the MI300X GPU for reference:

- **BF16 Compute**: 1.3 PFLOP/s
- **FP8 Compute**: 2.6 PFLOP/s
- **HBM**: 192GB HBM3E
- **HBM Bandwidth**: 5.3 TB/s
- **Power Consumption**: 750W

GEMM (General Matrix Multiply) is a fundamental operation in many machine learning and
scientific computing applications, particularly in training and inference of large
language models (LLMs). The MI300X GPU's support for bfloat16 (BF16) precision allows
for efficient computation while maintaining a balance between performance and numerical
stability.

## GEMM on MI300X

MI300X has a bfloat16 flops to memory ratio of 2.6 PFLOP/s / 5.3 TB/s = 490 flops/bytes.
This means that for every byte of memory accessed, the MI300X needs to perform at least
490 floating point operations to achieve peak performance. In the context of a GEMM
operation, this means that each matrix dimension should be at least 490. While training
transformer LLMs, this ratio is typically achieved by batching over the sequence length.
However, during inference using auto-regressive decoding when one token is generated
per step, the sequence length is typically 1, which means that the batch size
must be large enough (i.e. more than 490) to achieve the required flops to memory ratio.
As we will see in the benchmark results, the compute to memory ratio typically needs
to be even higher than 490 due to the overhead of the GEMM operation itself.

![CDNA GPU Architecture](/images/mi300x-cdna-architecture.png)

Matrix multiplication in the MI300X GPU is performed by mapping the GEMM operation to the
CDNA architecture, which is composed of multiple compute units (CUs) that work in parallel.
The matrix multiplication is divided into smaller tiles, which are then processed by the CUs.
The core operation performed by each CU is a Matrix fused-multiply-add (MFMA) operation,
which computes the product of two matrices and adds the result to a third matrix, each
of which is stored in the CU's register file. The V_MFMA_F32_BF16_BF16 instruction
performs a matrix multiplication of two bfloat16 matrices and accumulates the result
into a float32 matrix.

![CDNA Chiplet Architecture](/images/mi300x-chiplet.png)

The MI300X GPU is composed of 4 CDNA chiplets, each of which contains 76 CUs, each of
whcih containers 4 matrix cores. The chiplets are connected to each other using
Infinity Fabric links. The chiplets are connected to 2 HBM3 stacks each, creating
NUMA (Non-Uniform Memory Access) regions.

## Bfloat16 Format

Bfloat16 is a 16-bit floating point format that is designed to provide a good balance
between performance and numerical stability. It has the same exponent range as
float32, but only 7 bits of precision in the mantissa. This means that it can represent
a wide range of values, but with less precision than float32. The bfloat16 format is
commonly used in deep learning applications, particularly for training and inference of
large language models (LLMs). It allows for efficient computation while maintaining
numerical stability, as the exponent range is sufficient to represent the values
typically encountered in deep learning workloads.

![Bfloat16 Format](/images/bfloat16.png)

The exponent is stored in 8 bits, with the lowest representable value being
-126 and the highest representable value being 127. Accumulation is performed in
float32, which allows for higher precision in the final result. During mixed precision
training, the weights and input activations are typically cast to bfloat16 before
performing the GEMM operation. In particular, a common value of 1.0f corresponds to
an exponent of 127 in bfloat16, or in hex 0x7f00, where all bits of the exponent are
set to 1.

## Benchmark Results

![GEMM Benchmark](/images/mi300x-gemm.png)

The benchmark results show that the MI300X achieves a peak performance of 890 TFLOP/s
for bfloat16 GEMM operations, which is 68.4% of the theoretical peak performance of 1.3 PFLOP/s.
MI300X has a power limit of 750W, which becomes the limiting factor for the performance
of compute intensive operations like GEMM.

![GEMM Power and Clocks](/images/mi300x-power-and-clocks.png)

The power consumption of the MI300X during the benchmark is maxed out at 750W. During
the most power intensive benchmark, with a large GEMM operation with random bits,
the CU clocks are scaled down to 1.1 GHz to stay within the power limit.

Best performance is achieved with a compute intensity of 1365 flops/byte for square
4k by 4k matrices, which is significantly higher than the theoretical minimum of 490 flops/byte.

## The number of zeros matters

![GEMM Benchmark](/images/mi300x-gemm-zero-bits.png)

In this figure we sweep the number of zero bits in the input matrices to see how it
affects the performance of the GEMM operation. If the inputs are random normally distributed,
the performance is 645 TFLOP/s. If the inputs are all zeros, the performance in increases
to 890 TFLOP/s. Importantly, for common values from a function like torch.randn
with magnitudes between 0.1f and 1.0f, the exponent has 6 or 7 bits set to one because the
exponent is stored in two's complement format. If we scale the numbers down by a factor of
2^-119, we can cut the average number of exponents bits set to one down to 2. This results in a
performance of 780 TFLOP/s. This suggests that a huge speedup of 1.37x is possible by
paying close attention to the distribution of weight, activation, and delta values in the
input matrices.

## Analysis

Most practical workloads, especially inference scenarios with small batch sizes, operate
in the steep memory-bound region where performance scales linearly with operational intensity.
The data points clustering around 10-100 FLOP/byte represent typical LLM inference scenarios,
where the MI300X delivers only 5-100 TFLOP/sâ€”far below its theoretical peak. This suggests
that batching is incredibly important for achieving high inference performance on the MI300X,
especially for requests with large numbers of output tokens.

## PyTorch Benchmark Code

You can find the benchmark code on the [ScalarLM GEMM Github](https://github.com/tensorwavecloud/ScalarLM/blob/main/test/benchmark/pytorch/gemm.py).

Let's take a look at the code:

### Memcpy Sizes

```python
# List of memcpy sizes, in bytes, should be multiples of the page size
# Go up to the tensor size used in Llama 3 (4096 * 128256 * 4) = 2_101_346_304
memcpy_sizes = [ 2 ** i for i in range(12, 64) if 2 ** i <= 2_101_346_304 ]
```

This code sets up the sizes of the data to be copied. The sizes are powers of 2, starting from 4KB (2^12) and going up to
1.1GB (2^30). The sizes are chosen to be multiples of the page size, which is 4KB on most systems. The maximum size is
the size of the embedding tables in the Llama 3 8B model, which is 2_101_346_304 bytes (or 2.1GB). The benchmark is run for
each of these sizes, and the bandwidth is measured for each size. The results are plotted in the figure above.

### Benchmark Setup

Next, we set up the benchmark:

```python
def run_memcpy_benchmark():

    warmup()

    results = {}

    for size in tqdm(memcpy_sizes):
        results[size] = run_memcpy(size)

    return results
```

This function runs the memcpy benchmark. It first warms up the GPU by running a few iterations of the memcpy kernel
without measuring the time. This is done to ensure that the GPU is in a good state before running the benchmark. The
function then runs the memcpy kernel for each size in the `memcpy_sizes` list and measures the time taken to copy the data.

### Warmup

Warmup is pretty simple.

```python
def warmup():
    run_memcpy(4096)
```

This function runs the memcpy kernel with a size of 4KB (4096 bytes) to warm up the GPU. GPUs have startup times to
load the code, ramp up the clocks, etc. Running benchmarks without a warmup can lead to misleading results.

### Running Memcpy Peer PyTorch

The `run_memcpy` function is where the actual memcpy kernel is run. It uses PyTorch to allocate memory on the GPU and
copy data from one location to another. The function measures the time taken to copy the data and calculates the bandwidth
and other metrics.

The memcpy kernel is run for at least 1 second to get a good measurement of the bandwidth. The function uses PyTorch's
`copy_` method to copy data from one tensor to another. copy_ is the in-place version of the copy method, which means
that it modifies the destination tensor in place. This is more efficient than creating a new tensor for the result because it avoids
allocating memory for the result tensor.

```python
def run_memcpy(size):
    a = torch.zeros(size // 4, device=get_device(), dtype=torch.float32) # size is in bytes, so divide by 4 to get number of floats
    b = torch.zeros(size // 4, device=get_device(), dtype=torch.float32)

    # copy for at least 1 second
    barrier()

    start = get_event()
    end = get_event()

    start_time = time.time()

    start.record()
    iterations = 0
    while time.time() - start_time < 1:
        b.copy_(a)
        iterations += 1
    end.record()

    barrier()
    total_time = start.elapsed_time(end) * 1e-3 / iterations

    return {
        "operational_intensity": 1 / 4,  # 1 FLOP per 4 bytes
        "flop/s": size / 4 / total_time,
        "bytes": size,
        "time": total_time,
        "iterations": iterations,
        "bandwidth": size / total_time,
        "GB/s": size / total_time / 1e9,
    }
```

### Handling GPUs

When running on a GPU, the benchmark uses PyTorch's CUDA events to measure the time taken to copy the data. CUDA events are
used to measure the time taken to execute a kernel on the GPU. The `record` method is used to record the time at which
the event is recorded. The `elapsed_time` method is used to calculate the time taken to execute the kernel. The time
is measured in milliseconds, so we multiply by 1e-3 to convert to seconds. Using events is necessary because the GPU
is asynchronous, meaning that the CPU and GPU can run in parallel. The CPU can continue executing while the GPU is
copying data. This can lead to misleading results if the time taken to copy the data is not measured correctly.

In order to make the code cross-platform, we define a `get_event` function that returns a CUDA event if the GPU is available, or a CPU event
if the GPU is not available. The CPU event is a simple wrapper around the time module that records the time when the event is created and
calculates the elapsed time between two events.

```python
class CPUEvent:
    def __init__(self):
        self.time = 0

    def record(self):
        self.time = time.time()

    def elapsed_time(self, other):
        return (other.time - self.time) * 1000


def get_event():
    if torch.cuda.is_available():
        return torch.cuda.Event(enable_timing=True)
    else:
        return CPUEvent()


def barrier():
    if torch.cuda.is_available():
        torch.cuda.synchronize()
    else:
        pass

def get_device():
    if torch.cuda.is_available():
        return torch.device("cuda:0")
    else:
        return torch.device("cpu")
```


